---

title : "밑바닥부터 시작하는 딥러닝 - 5 오차역전파법(back propagation)"
date : 2019-08-04 10:00:00 + 0000
tags: DeepLearning
category: DeepLearning

---

# Intro
이번 포스트에서는 [밑바닥부터 시작하는 딥러닝](https://github.com/WegraLee/deep-learning-from-scratch) **오류 역전파법** 을 공부하며 잊어버리지 않기 위해 이해한 내용을 정리하였다.

***

# 1. 계산 그래프
- 계산 그래프(computational graph): 계산 과정을 그래프로 나타낸 것
- 그래프(graph): 복수의 노드(node)와 에지(edge)로 구성

## 1-1. 계산 그래프로 풀다
계산 그래프는 계산 과정을 노드와 화살표로 표현한다. 노드는 원(O)으로 표현하고 원 안에 연산 내용을 적는다. 또 계산 결과를 화살표 위에 적어, 각 노드의 계산 결과가 화살표 방향으로 전해지게 한다.

예시)
문제: 린구가 카페에서 카페모카 2잔, 케익 4개를 샀다. 카페모카는 1잔에 1500원, 케익은 1개에 500원이다. 소비세가 10%일 때 지불 금액을 구하라.

계산 그래프:
![딥러닝5](/assets/images/2019-08-04-밑바닥부터 시작하는 딥러닝 5/1.PNG){: width="800" height="600"}

계산 그래프를 이용한 문제 풀이는 다음 흐름으로 진행된다.
1. 계산 그래프를 구성한다.
2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다.
=> 왼쪽에서 오른쪽으로 진행하는 단계를 **순전파(forward propagation)** 라고 한다.
=> 오른쪽에서 왼쪽으로 진행하는 것은 **역전파(back propagation)** 라고 한다.

## 1-2. 국소적 계산
국소적 계산이란 전체에서 어떤 일이 벌어지든 상관 없이 자신과 관계된 국소 범위의 정보만으로 결과를 출력하는 것을 말한다.
계산 그래프에서 전체 계산이 아무리 복잡해도 각 단계에서 하는 일은 해당 노드의 국소적 계산이다.
=> 단순하지만, 그 결과를 전달함으로써 전체를 구성하는 복잡한 계산을 완성할 수 있다.

## 1-3. 계산 그래프의 장점
- 국소적 계산: 전체가 아무리 복잡해도 각 노드에서는 단순한 계산에 집중해서 문제를 단순화할 수 있다.
- 역전파를 통해 '미분'을 효율적으로 계산할 수 있다.
예를 들면, 카페모카를 x, 지불 금액을 L이라고 했을 때 카페모카 값이 아주 조금 올랐을 때, 지불 금액이 얼마나 증가하는지 알 수 있다($\frac{\partial L}{\partial x}$).
사과 가격에 대한 미분과 마찬가지로, 소비세에 대한 지불 금액의 미분, 사과 개수에 대한 지불 금액의 미분도 구할 수 있다.

***

# 2. 연쇄 법칙
국소적 미분을 전달하는 원리는 연쇄법칙(chain rule)에 따른 것이다.

## 2-1. 계산 그래프의 역전파
역전파의 간단한 예제를 살펴보자.
$y=f(x)$
위 수식의 역전파 그래프는 다음과 같다.
![딥러닝5](/assets/images/2019-08-04-밑바닥부터 시작하는 딥러닝 5/2.png){: width="450" height="350"}

역전파의 계산 절차는 신호 E에 노드의 국소적 미분을 곱한 후 앞 쪽 노드로 전달하는 것이다.
- 국소적 미분: y=f(x)의 미분
- 미분을 상류에서 전달된 값: 여기서는 E.
이러한 방식을 따르면 목표로 하는 미분 값을 효율적으로 구할 수 있다는 것이 이 전파의 핵심이다. 왜 이런일이 가능한가를 연쇄 법칙의 원리로 설명할 수 있다.


## 2-2. 연쇄 법칙이란
연쇄 법칙을 설명하려면 합성함수를 이해해야 한다. 합성 함수란 여러 함수로 구성되 함수이다.
예시)
$$z=t^2$$
$$t=x+y$$

연쇄 법칙은 합성 함수의 미분에 대한 성질이며, 다음과 같이 정의 된다.
**"합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다."**

$\frac{\partial z}{\partial x}$ ($x$에 대한 $z$의 미분)은 $\frac{\partial z}{\partial t}$($t$에 대한 $z$의 미분)과 $\frac{\partial t}{\partial x}$($x$에 대한 $t$의 미분)의 곱으로 나타낼 수 있다는 것이다.

$\frac{\partial z}{\partial x}$ = $\frac{\partial z}{\partial t}$$\frac{\partial t}{\partial x}$

연쇄 법칙을 통해 미분 $\frac{\partial z}{\partial x}$을 구해보자.
먼저 국소적 미분(편미분)을 구한다.
 $\frac{\partial z}{\partial t}$ = $2t$
 $\frac{\partial t}{\partial x}$ = $1$

그리고 최종적으로 두 국소적 미분을 곱하면 된다.
따라서 $\frac{\partial z}{\partial x}$ = $\frac{\partial z}{\partial t}$$\frac{\partial t}{\partial x}$ = $2t*1$ = $2(x+y)$

## 2-3. 연쇄 법칙과 계산 그래프
위의 연쇄 법칙 계산을 계산 그래프로 나타내면 다음과 같다.
![딥러닝5](/assets/images/2019-08-04-밑바닥부터 시작하는 딥러닝 5/3.png){: width="450" height="350"}

역전파는 오른쪽에서 왼쪽으로 신호를 전파하는데, 노드로 들어온 입력 신호에 그 노드이 국소적 미분(편미분)을 곱한 후 다음 노드로 전달한다. 예를 들어 $**2 노드에서 역전파를 보자.

입력은 $\frac{\partial z}{\partial z}$ 이며, 이에 국소적 미분인 $\frac{\partial z}{\partial t}$(순전파 시에 입력이 $t$이고, 출력이 $z$이므로 이 노드에서 국소적 미분은 $\frac{\partial z}{\partial t}$이다)를 곱하고 다음 노드로 넘긴다.

주목할 점은, 맨 왼쪽 역전파인데, 이 계산은 연쇄 법칙에 따르면 $\frac{\partial z}{\partial z}$$\frac{\partial z}{\partial t}$$\frac{\partial t}{\partial x}$ = $\frac{\partial z}{\partial t}$$\frac{\partial t}{\partial x}$ = $\frac{\partial z}{\partial x}$ 가 성립되어, '$x$에 대한 $z$의 미분'이 된다.
즉 역전파가 하는 일은 연쇄 법칙의 원리와 같다.

***

# 3. 역전파

## 3-1. 덧셈 노드의 역전파
덧셈 노드는 미분 값이 항상 1이 되기 때문에, 덧셈 노드의 역전파는 1을 곱하기만 할 뿐이므로 입력된 값(상류 값)을 그대로 다음 노드로 보내게 된다.

![딥러닝5](/assets/images/2019-08-04-밑바닥부터 시작하는 딥러닝 5/4.png){: width="700" height="500"}



가령 '10 + 5 = 15'라는 계산이 있고, 상류에서 1.3이라는 값이 흘러오는 것을 계산 그래프로 그리면 다음과 같다.
![딥러닝5](/assets/images/2019-08-04-밑바닥부터 시작하는 딥러닝 5/5.png){: width="700" height="500"}


## 3-2. 곱셈 노드의 역전파
$z=xy$라는 식으로 곱셈 노드의 역전파를 알아보자. 이 식의 미분과 계산 그래프는 다음과 같다.
$\frac{\partial z}{\partial x}=y$
$\frac{\partial z}{\partial y}=x$

![딥러닝5](/assets/images/2019-08-04-밑바닥부터 시작하는 딥러닝 5/6.png){: width="700" height="500"}

곱셈 노드의 역전파는 입력 된 값(상류 값)에 순전파 때의 입력 신호들을 '서로 바꾼 값'을 곱해서 다음 노드로 보낸다. 서로 바꾼 값이란 순전파 때 $x$였다면 역전파에서는 $y$, 순전파 때 $y$였다면 역전파에서는 $x$로 바꾼 다는 의미이다.

구체적으로 가령 '10 x 5 = 50'이라는 계산이 있고, 역전파 때 상류에서 1.3 값이 흘러온다고 하자. 이를 계산 그래프로 그리면 다음과 같다.

![딥러닝5](/assets/images/2019-08-04-밑바닥부터 시작하는 딥러닝 5/7.png){: width="700" height="500"}

** 덧셈의 역전파에서는 상류의 값을 그대로 흘려보내서 순방향 입력 신호의 값은 필요하지 않았지만, 곱셈의 역전파는 순방향 입력 신호의 값이 필요하기 떄문에 곱셈 노드를 구현할 때는 순전파의 입력 신호를 변수에 저장해둔다.


***

# 4. 단순한 계층 구현하기
'사과 쇼핑' 예를 파이썬으로 구현하는 예시이다. 여기에서는 계산 그래프의 곱셈 노드를 'MulLayer', 덧셈 노드를 'AddLayer'라는 이름으로 구현한다.

## 4-1. 곱셈 계층
```
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None

    def forward(self, x, y):
        self.x = x
        self.y = y                
        out = x * y

        return out

    def backward(self, dout):
        dx = dout * self.y  # x와 y를 바꾼다.
        dy = dout * self.x

        return dx, dy
```

# 4-2. 덧셈 계층
```
class AddLayer:
    def __init__(self):
        pass

    def forward(self, x, y):
        out = x + y

        return out

    def backward(self, dout):
        dx = dout * 1
        dy = dout * 1

        return dx, dy
```

## 4-3. 곱셈 계층과 덧셈 계층을 사용하여, 사과 2개와 귤 3개를 구매하는 상황 구현
```
# coding: utf-8
from layer_naive import *

apple = 100
apple_num = 2
orange = 150
orange_num = 3
tax = 1.1

# layer
mul_apple_layer = MulLayer()
mul_orange_layer = MulLayer()
add_apple_orange_layer = AddLayer()
mul_tax_layer = MulLayer()

# forward
apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)
orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)
all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)
price = mul_tax_layer.forward(all_price, tax)  # (4)

# backward
dprice = 1
dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)
dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)
dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)
dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)

print("price:", int(price)) # 715
print("dApple:", dapple) # 2.2
print("dApple_num:", int(dapple_num)) # 110
print("dOrange:", dorange) # 3.3
print("dOrange_num:", int(dorange_num)) # 165
print("dTax:", dtax) # 650

```

backward()의 호출 순서는 forward()와 반대이다. 또 backward()가 받는 인수는 '순전파의 출력에 대한 미분'이다. 가령, mul_apple_layer라는 곱셈 계층은 순전파 때는 apple_price를 출력하지만, 역전파 때는 apple_price의 미분 값인 dapple_price를 인수로 받는다.

***

# 5. 활성화 함수 계층 구현하기

## 5-1. ReLU 계층
활성화 함수로 사용되는 ReLU의 수식은 다음과 같다.

```
class Relu:
    def __init__(self):
        self.mask = None # mask는 True/False로 구성된 넘파이 배열

    def forward(self, x):
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0

        return out

    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout

        return dx
```

ReLU 클래스킄 mask 라는 인스턴스 변수를 가진다. mask는 True/False로 구성된 넘파이 배열로, 순전파의 입력인 x의 원소 값이 0 이하인 인덱스는 True, 그 외는 False로 유지한다.
ReLU는 순전파 때의 입력 값이 0 이하면 역전파 때의 값이 0이 되어야 하므로, 역전파때는 순전파때 만들어둔 mask를 써서 mask의 원소가 True인 곳에는 상류에서 전파된 dout을 0으로 설정한다.

## 5-2. Sigmoid 계층
시그모이드 함수의 수식은 다음과 같다.
$y=1/(1+exp(-x))$

```
class Sigmoid:
    def __init__(self):
        self.out = None

    def forward(self, x):
        out = 1 / (1 + np.exp(-x))
        self.out = out
        return out

    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out

        return dx
```

***

# 6. Affine/Softmax 계층 구현하기
